{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWPivw5Ss1Hk"
   },
   "source": [
    "# 7章\n",
    "- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LvCX0ZnVJ1WD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/root/work/chap7\n"
     ]
    }
   ],
   "source": [
    "# 7-1\n",
    "!mkdir chap7\n",
    "%cd ./chap7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0iMot3XGIhtD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.18.0 in /usr/local/lib/python3.8/dist-packages (4.18.0)\n",
      "Requirement already satisfied: fugashi==1.1.0 in /usr/local/lib/python3.8/dist-packages (1.1.0)\n",
      "Requirement already satisfied: ipadic==1.0.0 in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
      "Requirement already satisfied: pytorch-lightning==1.6.1 in /usr/local/lib/python3.8/dist-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (0.0.49)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (2022.3.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.18.0) (0.5.1)\n",
      "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (0.3.2)\n",
      "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (1.11.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (0.8.2)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (2022.3.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (4.1.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.18.0) (3.0.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (2.1.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.43.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.34.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (45.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (2.6.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.20.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.18.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.18.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.18.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.18.0) (1.26.9)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.18.0) (8.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.18.0) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.18.0) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (4.11.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 7-2\n",
    "!pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "87bW8wO5IhtF"
   },
   "outputs": [],
   "source": [
    "# 7-3\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5HFcRL7nnhbX"
   },
   "outputs": [],
   "source": [
    "# 7-4\n",
    "class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        # BertModelのロード\n",
    "        self.bert = BertModel.from_pretrained(model_name) \n",
    "        # 線形変換を初期化しておく\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_labels\n",
    "        ) \n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None, \n",
    "        attention_mask=None, \n",
    "        token_type_ids=None, \n",
    "        labels=None\n",
    "    ):\n",
    "        # データを入力しBERTの最終層の出力を得る。\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "        last_hidden_state = bert_output.last_hidden_state\n",
    "        \n",
    "        # [PAD]以外のトークンで隠れ状態の平均をとる\n",
    "        averaged_hidden_state = \\\n",
    "            (last_hidden_state*attention_mask.unsqueeze(-1)).sum(1) \\\n",
    "            / attention_mask.sum(1, keepdim=True)\n",
    "        \n",
    "        # 線形変換\n",
    "        scores = self.linear(averaged_hidden_state) \n",
    "        \n",
    "        # 出力の形式を整える。\n",
    "        output = {'logits': scores}\n",
    "\n",
    "        # labelsが入力に含まれていたら、損失を計算し出力する。\n",
    "        if labels is not None: \n",
    "            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n",
    "            output['loss'] = loss\n",
    "            \n",
    "        # 属性でアクセスできるようにする。\n",
    "        output = type('bert_output', (object,), output) \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RbWDC5z4x_kP"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3357859e75ad4a2e961d5131d24a5235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/252k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace17d8038024f05a7aa3ee56b4fb1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116d386ec402427698cc456f1254dbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/479 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e716c9f2c8f41b08ac306c20c2a7f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 7-5\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_scml = BertForSequenceClassificationMultiLabel(\n",
    "    MODEL_NAME, num_labels=2\n",
    ") \n",
    "bert_scml = bert_scml.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V_ep4ddFjz-O"
   },
   "outputs": [],
   "source": [
    "# 7-6\n",
    "text_list = [\n",
    "    '今日の仕事はうまくいったが、体調があまり良くない。',\n",
    "    '昨日は楽しかった。'\n",
    "]\n",
    "\n",
    "labels_list = [\n",
    "    [1, 1],\n",
    "    [0, 1]\n",
    "]\n",
    "\n",
    "# データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list, \n",
    "    padding='longest',  \n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "labels = torch.tensor(labels_list).cuda()\n",
    "\n",
    "# BERTへデータを入力し分類スコアを得る。\n",
    "with torch.no_grad():\n",
    "    output = bert_scml(**encoding)\n",
    "scores = output.logits\n",
    "\n",
    "# スコアが正ならば、そのカテゴリーを選択する。\n",
    "labels_predicted = ( scores > 0 ).int()\n",
    "\n",
    "# 精度の計算\n",
    "num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n",
    "accuracy = num_correct/labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QrXA5KgXmX-m"
   },
   "outputs": [],
   "source": [
    "# 7-7\n",
    "# データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list, \n",
    "    padding='longest',  \n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding['labels'] = torch.tensor(labels_list) # 入力にlabelsを含める。\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "output = bert_scml(**encoding)\n",
    "loss = output.loss # 損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HJ9Tbr6PIhtF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-09 09:41:54--  https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\n",
      "Resolving s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)... 52.219.8.132\n",
      "Connecting to s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)|52.219.8.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 722777 (706K) [application/zip]\n",
      "Saving to: ‘chABSA-dataset.zip.1’\n",
      "\n",
      "chABSA-dataset.zip. 100%[===================>] 705.84K   953KB/s    in 0.7s    \n",
      "\n",
      "2022-05-09 09:41:55 (953 KB/s) - ‘chABSA-dataset.zip.1’ saved [722777/722777]\n",
      "\n",
      "Archive:  chABSA-dataset.zip\n",
      "   creating: chABSA-dataset/\n",
      "  inflating: chABSA-dataset/.DS_Store  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/chABSA-dataset/\n",
      "  inflating: __MACOSX/chABSA-dataset/._.DS_Store  \n",
      " extracting: chABSA-dataset/.gitkeep  \n",
      "  inflating: chABSA-dataset/e00008_ann.json  \n",
      "  inflating: chABSA-dataset/e00017_ann.json  \n",
      "  inflating: chABSA-dataset/e00024_ann.json  \n",
      "  inflating: chABSA-dataset/e00026_ann.json  \n",
      "  inflating: chABSA-dataset/e00030_ann.json  \n",
      "  inflating: chABSA-dataset/e00033_ann.json  \n",
      "  inflating: chABSA-dataset/e00034_ann.json  \n",
      "  inflating: chABSA-dataset/e00035_ann.json  \n",
      "  inflating: chABSA-dataset/e00037_ann.json  \n",
      "  inflating: chABSA-dataset/e00051_ann.json  \n",
      "  inflating: chABSA-dataset/e00053_ann.json  \n",
      "  inflating: chABSA-dataset/e00058_ann.json  \n",
      "  inflating: chABSA-dataset/e00069_ann.json  \n",
      "  inflating: chABSA-dataset/e00091_ann.json  \n",
      "  inflating: chABSA-dataset/e00107_ann.json  \n",
      "  inflating: chABSA-dataset/e00114_ann.json  \n",
      "  inflating: chABSA-dataset/e00146_ann.json  \n",
      "  inflating: chABSA-dataset/e00168_ann.json  \n",
      "  inflating: chABSA-dataset/e00184_ann.json  \n",
      "  inflating: chABSA-dataset/e00194_ann.json  \n",
      "  inflating: chABSA-dataset/e00273_ann.json  \n",
      "  inflating: chABSA-dataset/e00308_ann.json  \n",
      "  inflating: chABSA-dataset/e00343_ann.json  \n",
      "  inflating: chABSA-dataset/e00354_ann.json  \n",
      "  inflating: chABSA-dataset/e00380_ann.json  \n",
      "  inflating: chABSA-dataset/e00406_ann.json  \n",
      "  inflating: chABSA-dataset/e00435_ann.json  \n",
      "  inflating: chABSA-dataset/e00457_ann.json  \n",
      "  inflating: chABSA-dataset/e00465_ann.json  \n",
      "  inflating: chABSA-dataset/e00501_ann.json  \n",
      "  inflating: chABSA-dataset/e00534_ann.json  \n",
      "  inflating: chABSA-dataset/e00541_ann.json  \n",
      "  inflating: chABSA-dataset/e00547_ann.json  \n",
      "  inflating: chABSA-dataset/e00563_ann.json  \n",
      "  inflating: chABSA-dataset/e00603_ann.json  \n",
      "  inflating: chABSA-dataset/e00686_ann.json  \n",
      "  inflating: chABSA-dataset/e00693_ann.json  \n",
      "  inflating: chABSA-dataset/e00694_ann.json  \n",
      "  inflating: chABSA-dataset/e00721_ann.json  \n",
      "  inflating: chABSA-dataset/e00772_ann.json  \n",
      "  inflating: chABSA-dataset/e00787_ann.json  \n",
      "  inflating: chABSA-dataset/e00810_ann.json  \n",
      "  inflating: chABSA-dataset/e00832_ann.json  \n",
      "  inflating: chABSA-dataset/e00838_ann.json  \n",
      "  inflating: chABSA-dataset/e00840_ann.json  \n",
      "  inflating: chABSA-dataset/e00842_ann.json  \n",
      "  inflating: chABSA-dataset/e00858_ann.json  \n",
      "  inflating: chABSA-dataset/e00877_ann.json  \n",
      "  inflating: chABSA-dataset/e00886_ann.json  \n",
      "  inflating: chABSA-dataset/e00909_ann.json  \n",
      "  inflating: chABSA-dataset/e00911_ann.json  \n",
      "  inflating: chABSA-dataset/e00939_ann.json  \n",
      "  inflating: chABSA-dataset/e00962_ann.json  \n",
      "  inflating: chABSA-dataset/e00976_ann.json  \n",
      "  inflating: chABSA-dataset/e01018_ann.json  \n",
      "  inflating: chABSA-dataset/e01043_ann.json  \n",
      "  inflating: chABSA-dataset/e01054_ann.json  \n",
      "  inflating: chABSA-dataset/e01097_ann.json  \n",
      "  inflating: chABSA-dataset/e01118_ann.json  \n",
      "  inflating: chABSA-dataset/e01151_ann.json  \n",
      "  inflating: chABSA-dataset/e01156_ann.json  \n",
      "  inflating: chABSA-dataset/e01173_ann.json  \n",
      "  inflating: chABSA-dataset/e01183_ann.json  \n",
      "  inflating: chABSA-dataset/e01197_ann.json  \n",
      "  inflating: chABSA-dataset/e01216_ann.json  \n",
      "  inflating: chABSA-dataset/e01230_ann.json  \n",
      "  inflating: chABSA-dataset/e01244_ann.json  \n",
      "  inflating: chABSA-dataset/e01249_ann.json  \n",
      "  inflating: chABSA-dataset/e01260_ann.json  \n",
      "  inflating: chABSA-dataset/e01334_ann.json  \n",
      "  inflating: chABSA-dataset/e01364_ann.json  \n",
      "  inflating: chABSA-dataset/e01398_ann.json  \n",
      "  inflating: chABSA-dataset/e01402_ann.json  \n",
      "  inflating: chABSA-dataset/e01425_ann.json  \n",
      "  inflating: chABSA-dataset/e01436_ann.json  \n",
      "  inflating: chABSA-dataset/e01462_ann.json  \n",
      "  inflating: chABSA-dataset/e01469_ann.json  \n",
      "  inflating: chABSA-dataset/e01506_ann.json  \n",
      "  inflating: chABSA-dataset/e01528_ann.json  \n",
      "  inflating: chABSA-dataset/e01529_ann.json  \n",
      "  inflating: chABSA-dataset/e01533_ann.json  \n",
      "  inflating: chABSA-dataset/e01546_ann.json  \n",
      "  inflating: chABSA-dataset/e01585_ann.json  \n",
      "  inflating: chABSA-dataset/e01620_ann.json  \n",
      "  inflating: chABSA-dataset/e01624_ann.json  \n",
      "  inflating: chABSA-dataset/e01629_ann.json  \n",
      "  inflating: chABSA-dataset/e01635_ann.json  \n",
      "  inflating: chABSA-dataset/e01703_ann.json  \n",
      "  inflating: chABSA-dataset/e01719_ann.json  \n",
      "  inflating: chABSA-dataset/e01731_ann.json  \n",
      "  inflating: chABSA-dataset/e01740_ann.json  \n",
      "  inflating: chABSA-dataset/e01743_ann.json  \n",
      "  inflating: chABSA-dataset/e01764_ann.json  \n",
      "  inflating: chABSA-dataset/e01794_ann.json  \n",
      "  inflating: chABSA-dataset/e01798_ann.json  \n",
      "  inflating: chABSA-dataset/e01813_ann.json  \n",
      "  inflating: chABSA-dataset/e01849_ann.json  \n",
      "  inflating: chABSA-dataset/e01862_ann.json  \n",
      "  inflating: chABSA-dataset/e01865_ann.json  \n",
      "  inflating: chABSA-dataset/e01903_ann.json  \n",
      "  inflating: chABSA-dataset/e01904_ann.json  \n",
      "  inflating: chABSA-dataset/e01933_ann.json  \n",
      "  inflating: chABSA-dataset/e01946_ann.json  \n",
      "  inflating: chABSA-dataset/e01968_ann.json  \n",
      "  inflating: chABSA-dataset/e01972_ann.json  \n",
      "  inflating: chABSA-dataset/e01974_ann.json  \n",
      "  inflating: chABSA-dataset/e01987_ann.json  \n",
      "  inflating: chABSA-dataset/e01992_ann.json  \n",
      "  inflating: chABSA-dataset/e02009_ann.json  \n",
      "  inflating: chABSA-dataset/e02049_ann.json  \n",
      "  inflating: chABSA-dataset/e02105_ann.json  \n",
      "  inflating: chABSA-dataset/e02150_ann.json  \n",
      "  inflating: chABSA-dataset/e02152_ann.json  \n",
      "  inflating: chABSA-dataset/e02214_ann.json  \n",
      "  inflating: chABSA-dataset/e02241_ann.json  \n",
      "  inflating: chABSA-dataset/e02246_ann.json  \n",
      "  inflating: chABSA-dataset/e02289_ann.json  \n",
      "  inflating: chABSA-dataset/e02353_ann.json  \n",
      "  inflating: chABSA-dataset/e02367_ann.json  \n",
      "  inflating: chABSA-dataset/e02380_ann.json  \n",
      "  inflating: chABSA-dataset/e02382_ann.json  \n",
      "  inflating: chABSA-dataset/e02390_ann.json  \n",
      "  inflating: chABSA-dataset/e02414_ann.json  \n",
      "  inflating: chABSA-dataset/e02423_ann.json  \n",
      "  inflating: chABSA-dataset/e02505_ann.json  \n",
      "  inflating: chABSA-dataset/e02525_ann.json  \n",
      "  inflating: chABSA-dataset/e02530_ann.json  \n",
      "  inflating: chABSA-dataset/e02544_ann.json  \n",
      "  inflating: chABSA-dataset/e02547_ann.json  \n",
      "  inflating: chABSA-dataset/e02563_ann.json  \n",
      "  inflating: chABSA-dataset/e02567_ann.json  \n",
      "  inflating: chABSA-dataset/e02608_ann.json  \n",
      "  inflating: chABSA-dataset/e02627_ann.json  \n",
      "  inflating: chABSA-dataset/e02632_ann.json  \n",
      "  inflating: chABSA-dataset/e02643_ann.json  \n",
      "  inflating: chABSA-dataset/e02673_ann.json  \n",
      "  inflating: chABSA-dataset/e02682_ann.json  \n",
      "  inflating: chABSA-dataset/e02732_ann.json  \n",
      "  inflating: chABSA-dataset/e02825_ann.json  \n",
      "  inflating: chABSA-dataset/e02837_ann.json  \n",
      "  inflating: chABSA-dataset/e02889_ann.json  \n",
      "  inflating: chABSA-dataset/e02905_ann.json  \n",
      "  inflating: chABSA-dataset/e02946_ann.json  \n",
      "  inflating: chABSA-dataset/e03128_ann.json  \n",
      "  inflating: chABSA-dataset/e03226_ann.json  \n",
      "  inflating: chABSA-dataset/e03236_ann.json  \n",
      "  inflating: chABSA-dataset/e03267_ann.json  \n",
      "  inflating: chABSA-dataset/e03275_ann.json  \n",
      "  inflating: chABSA-dataset/e03281_ann.json  \n",
      "  inflating: chABSA-dataset/e03367_ann.json  \n",
      "  inflating: chABSA-dataset/e03398_ann.json  \n",
      "  inflating: chABSA-dataset/e03401_ann.json  \n",
      "  inflating: chABSA-dataset/e03472_ann.json  \n",
      "  inflating: chABSA-dataset/e03505_ann.json  \n",
      "  inflating: chABSA-dataset/e03556_ann.json  \n",
      "  inflating: chABSA-dataset/e03566_ann.json  \n",
      "  inflating: chABSA-dataset/e03580_ann.json  \n",
      "  inflating: chABSA-dataset/e03582_ann.json  \n",
      "  inflating: chABSA-dataset/e03584_ann.json  \n",
      "  inflating: chABSA-dataset/e03601_ann.json  \n",
      "  inflating: chABSA-dataset/e03641_ann.json  \n",
      "  inflating: chABSA-dataset/e03693_ann.json  \n",
      "  inflating: chABSA-dataset/e03723_ann.json  \n",
      "  inflating: chABSA-dataset/e03784_ann.json  \n",
      "  inflating: chABSA-dataset/e03929_ann.json  \n",
      "  inflating: chABSA-dataset/e03991_ann.json  \n",
      "  inflating: chABSA-dataset/e04078_ann.json  \n",
      "  inflating: chABSA-dataset/e04091_ann.json  \n",
      "  inflating: chABSA-dataset/e04123_ann.json  \n",
      "  inflating: chABSA-dataset/e04136_ann.json  \n",
      "  inflating: chABSA-dataset/e04147_ann.json  \n",
      "  inflating: chABSA-dataset/e04191_ann.json  \n",
      "  inflating: chABSA-dataset/e04242_ann.json  \n",
      "  inflating: chABSA-dataset/e04273_ann.json  \n",
      "  inflating: chABSA-dataset/e04291_ann.json  \n",
      "  inflating: chABSA-dataset/e04298_ann.json  \n",
      "  inflating: chABSA-dataset/e04304_ann.json  \n",
      "  inflating: chABSA-dataset/e04319_ann.json  \n",
      "  inflating: chABSA-dataset/e04329_ann.json  \n",
      "  inflating: chABSA-dataset/e04331_ann.json  \n",
      "  inflating: chABSA-dataset/e04360_ann.json  \n",
      "  inflating: chABSA-dataset/e04503_ann.json  \n",
      "  inflating: chABSA-dataset/e04509_ann.json  \n",
      "  inflating: chABSA-dataset/e04727_ann.json  \n",
      "  inflating: chABSA-dataset/e04768_ann.json  \n",
      "  inflating: chABSA-dataset/e04844_ann.json  \n",
      "  inflating: chABSA-dataset/e04856_ann.json  \n",
      "  inflating: chABSA-dataset/e04858_ann.json  \n",
      "  inflating: chABSA-dataset/e04867_ann.json  \n",
      "  inflating: chABSA-dataset/e04877_ann.json  \n",
      "  inflating: chABSA-dataset/e04948_ann.json  \n",
      "  inflating: chABSA-dataset/e04976_ann.json  \n",
      "  inflating: chABSA-dataset/e04995_ann.json  \n",
      "  inflating: chABSA-dataset/e05011_ann.json  \n",
      "  inflating: chABSA-dataset/e05110_ann.json  \n",
      "  inflating: chABSA-dataset/e05145_ann.json  \n",
      "  inflating: chABSA-dataset/e05155_ann.json  \n",
      "  inflating: chABSA-dataset/e05167_ann.json  \n",
      "  inflating: chABSA-dataset/e05302_ann.json  \n",
      "  inflating: chABSA-dataset/e05319_ann.json  \n",
      "  inflating: chABSA-dataset/e05322_ann.json  \n",
      "  inflating: chABSA-dataset/e05346_ann.json  \n",
      "  inflating: chABSA-dataset/e05452_ann.json  \n",
      "  inflating: chABSA-dataset/e05469_ann.json  \n",
      "  inflating: chABSA-dataset/e05480_ann.json  \n",
      "  inflating: chABSA-dataset/e05593_ann.json  \n",
      "  inflating: chABSA-dataset/e05629_ann.json  \n",
      "  inflating: chABSA-dataset/e05714_ann.json  \n",
      "  inflating: chABSA-dataset/e05737_ann.json  \n",
      "  inflating: chABSA-dataset/e07801_ann.json  \n",
      "  inflating: chABSA-dataset/e21200_ann.json  \n",
      "  inflating: chABSA-dataset/e21261_ann.json  \n",
      "  inflating: chABSA-dataset/e23818_ann.json  \n",
      "  inflating: chABSA-dataset/e25665_ann.json  \n",
      "  inflating: chABSA-dataset/e26332_ann.json  \n",
      "  inflating: chABSA-dataset/e26443_ann.json  \n",
      "  inflating: chABSA-dataset/e26454_ann.json  \n",
      "  inflating: chABSA-dataset/e26713_ann.json  \n",
      "  inflating: chABSA-dataset/e26914_ann.json  \n",
      "  inflating: chABSA-dataset/e27050_ann.json  \n",
      "  inflating: chABSA-dataset/e27633_ann.json  \n",
      "  inflating: chABSA-dataset/e27759_ann.json  \n",
      "  inflating: chABSA-dataset/e30066_ann.json  \n",
      "  inflating: chABSA-dataset/e30085_ann.json  \n",
      "  inflating: chABSA-dataset/e30479_ann.json  \n",
      "  inflating: chABSA-dataset/e30746_ann.json  \n",
      "  inflating: chABSA-dataset/e32161_ann.json  \n",
      "  inflating: chABSA-dataset/e32189_ann.json  \n",
      "  inflating: chABSA-dataset/e32458_ann.json  \n",
      "  inflating: chABSA-dataset/e33009_ann.json  \n"
     ]
    }
   ],
   "source": [
    "# 7-8\n",
    "# データのダウンロード\n",
    "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\n",
    "# データの解凍\n",
    "!unzip chABSA-dataset.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zgXcOtz6fLge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'sentence': '当期におけるわが国経済は、景気は緩やかな回復基調が続き、設備投資の持ち直し等を背景に企業収益は改善しているものの、海外では、資源国等を中心に不透明な状況が続き、為替が急激に変動するなど、依然として先行きが見通せない状況で推移した', 'opinions': [{'target': 'わが国経済', 'category': 'NULL#general', 'polarity': 'neutral', 'from': 6, 'to': 11}, {'target': '景気', 'category': 'NULL#general', 'polarity': 'positive', 'from': 13, 'to': 15}, {'target': '設備投資', 'category': 'NULL#general', 'polarity': 'positive', 'from': 28, 'to': 32}, {'target': '企業収益', 'category': 'NULL#general', 'polarity': 'positive', 'from': 42, 'to': 46}, {'target': '資源国等', 'category': 'NULL#general', 'polarity': 'neutral', 'from': 62, 'to': 66}, {'target': '為替', 'category': 'NULL#general', 'polarity': 'negative', 'from': 80, 'to': 82}]}\n"
     ]
    }
   ],
   "source": [
    "# 7-9\n",
    "data = json.load(open('chABSA-dataset/e00030_ann.json'))\n",
    "print( data['sentences'][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l33ix4WDIhtG"
   },
   "outputs": [],
   "source": [
    "# 7-10\n",
    "category_id = {'negative':0, 'neutral':1 , 'positive':2}\n",
    "\n",
    "dataset = []\n",
    "for file in glob.glob('chABSA-dataset/*.json'):\n",
    "    data = json.load(open(file))\n",
    "    # 各データから文章（text）を抜き出し、ラベル（'labels'）を作成\n",
    "    for sentence in data['sentences']:\n",
    "        text = sentence['sentence'] \n",
    "        labels = [0,0,0]\n",
    "        for opinion in sentence['opinions']:\n",
    "            labels[category_id[opinion['polarity']]] = 1\n",
    "        sample = {'text': text, 'labels': labels}\n",
    "        dataset.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "k4Na8gOPHhya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '当社グループの当連結会計年度の業績は、売上高は149,525百万円（前連結会計年度比3,717百万円増、2.5％増）、営業利益は5,029百万円（前連結会計年度比809百万円増、19.2％増）、経常利益は4,475百万円（前連結会計年度比873百万円増、24.3％増）、親会社株主に帰属する当期純利益は3,990百万円（前連結会計年度比598百万円増、17.6％増）となり、昨年に引き続き増収増益となりました', 'labels': [0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 7-11\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "igPtmux1IhtI"
   },
   "outputs": [],
   "source": [
    "# 7-12\n",
    "# トークナイザのロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 各データの形式を整える\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "for sample in dataset:\n",
    "    text = sample['text']\n",
    "    labels = sample['labels']\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    encoding['labels'] = labels\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader) \n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n",
    "\n",
    "#　データセットからデータローダを作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=32, shuffle=True\n",
    ") \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9y3dO-kBIhtI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Missing logger folder: /home/root/work/chap7/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                    | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | bert_scml | BertForSequenceClassificationMultiLabel | 110 M \n",
      "----------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.479   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00c1b057d4140fb95f259f7489d4053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:1444: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `test(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at /home/root/work/chap7/model/epoch=3-step=460.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/root/work/chap7/model/epoch=3-step=460.ckpt\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7292a6aa0e84e4694d43a59cb8c036a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        accuracy            0.8848979473114014\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# 7-13\n",
    "class BertForSequenceClassificationMultiLabel_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        self.bert_scml = BertForSequenceClassificationMultiLabel(\n",
    "            model_name, num_labels=num_labels\n",
    "        ) \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels')\n",
    "        output = self.bert_scml(**batch)\n",
    "        scores = output.logits\n",
    "        labels_predicted = ( scores > 0 ).int()\n",
    "        num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n",
    "        accuracy = num_correct/scores.size(0)\n",
    "        self.log('accuracy', accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=5,\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassificationMultiLabel_pl(\n",
    "    MODEL_NAME, \n",
    "    num_labels=3, \n",
    "    lr=1e-5\n",
    ")\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "My3WI8Qd7yVJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "入力：今期は売り上げが順調に推移したが、株価は低迷の一途を辿っている。\n",
      "出力：[1, 0, 1]\n",
      "--\n",
      "入力：昨年から黒字が減少した。\n",
      "出力：[1, 0, 0]\n",
      "--\n",
      "入力：今日の飲み会は楽しかった。\n",
      "出力：[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 7-14\n",
    "# 入力する文章\n",
    "text_list = [\n",
    "    \"今期は売り上げが順調に推移したが、株価は低迷の一途を辿っている。\",\n",
    "    \"昨年から黒字が減少した。\",\n",
    "    \"今日の飲み会は楽しかった。\"\n",
    "]\n",
    "\n",
    "# モデルのロード\n",
    "best_model_path = checkpoint.best_model_path\n",
    "model = BertForSequenceClassificationMultiLabel_pl.load_from_checkpoint(best_model_path)\n",
    "bert_scml = model.bert_scml.cuda()\n",
    "\n",
    "# データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list, \n",
    "    padding = 'longest',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "# BERTへデータを入力し分類スコアを得る。\n",
    "with torch.no_grad():\n",
    "    output = bert_scml(**encoding)\n",
    "scores = output.logits\n",
    "labels_predicted = ( scores > 0 ).int().cpu().numpy().tolist()\n",
    "\n",
    "# 結果を表示\n",
    "for text, label in zip(text_list, labels_predicted):\n",
    "    print('--')\n",
    "    print(f'入力：{text}')\n",
    "    print(f'出力：{label}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chapter7.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/stockmarkteam/bert-book/blob/master/Chapter7.ipynb",
     "timestamp": 1630574288605
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
